{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4\n",
    "\n",
    "In this tutorial, we provide a demo on how to use FAIRSteam to generate ready-to-model datasets in both TensorFlow Dataset format and DataFrame format. Then, we provide 2 example models that take these modeling data as inputs and make either multi-class or binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import FAIRStream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv pool folder directory\n",
    "csv_pool_path = '/Users/jiaxingqiu/Documents/CAMA_projects/BSI/code/projects/csv_pool'\n",
    "# current experiment working directory\n",
    "work_dir = '/Users/jiaxingqiu/Documents/CAMA_projects/BSI/code/projects/case_txp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a FAIRStream object instance for BSI project\n",
    "bsi_stream = FAIRStream.FAIRStream(work_dir)\n",
    "# take a look at dictionaries in engineer's hands\n",
    "#bsi_stream.engineer.csv_source_dict\n",
    "bsi_stream.engineer.variable_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new UVA data to CSV pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( bsi_stream.engineer.csv_source_dict )\n",
    "# bsi_stream.querier.create_csv_pool(csv_pool_dir = csv_pool_path,\n",
    "#                                   source_key=\"uvanew\",\n",
    "#                                   file_key=\"vital\",\n",
    "#                                   sep=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsi_stream.querier.create_csv_pool(csv_pool_dir = csv_pool_path,\n",
    "#                                   source_key=\"uvanew\",\n",
    "#                                   file_key=\"lab\",\n",
    "#                                   sep=\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an episode (notice that the engineer now has new attributes)\n",
    "bsi_stream.engineer.DefineEpisode(input_time_len=2*24*60, # using vital signs and labs 4 days prior to a culture \n",
    "                                  output_time_len=24*60, # predict one time unit into the future\n",
    "                                  time_resolution=60, # aggregate minutely data to one row per hour \n",
    "                                  time_lag=0,  # no time lag between predictors and response\n",
    "                                  anchor_gap=7*24*60) # the minimum distance between two episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bsi_stream.engineer.episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MVTS (multi-variable time series) data objects\n",
    "- train_df_imputed, valid_df_imputed and test_df_imputed are dataframes\n",
    "- train_tfds, valid_tfds and test_tfds are tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build MVTS dataframe or tfds  (notice that the engineer now has new attributes)\n",
    "bsi_stream.engineer.BuildMVTS(csv_pool_path, \n",
    "                              nsbj = 100, # number of subjects / patients to sample from the pool \n",
    "                              replace=False, # sample with replacement or not \n",
    "                              valid_frac = 0.2, # fraction of number of subjects in validation dataset\n",
    "                              test_frac = 0.1, # fraction of number of subjects in left-out test dataset\n",
    "                              batch_size = 64, # batch size (usually 32,64,128..)\n",
    "                              impute_input='median', # imputation on predictors\n",
    "                              impute_output='median',\n",
    "                              sep=\"_\")# imputation on response (no need in BSI project)\n",
    "# please see the end of console \n",
    "# --- Success! Engineer has updated attributes --- train_df_imputed, valid_df_imputed and test_df_imputed. \n",
    "# --- Success! Engineer has updated attributes --- train_tfds, valid_tfds and test_tfds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat( [bsi_stream.engineer.train_df, bsi_stream.engineer.train_df], axis=0)\n",
    "all_df = pd.concat( [all_df, bsi_stream.engineer.test_df], axis=0)\n",
    "all_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_new = all_df.loc[all_df.__uid.str.startswith(\"uvanew_\"),]\n",
    "print(all_df_new.describe())\n",
    "all_df_old = all_df.loc[all_df.__uid.str.startswith(\"uva_\"),]\n",
    "print(all_df_old.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print all the attributes of the engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bsi_stream.engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsi_stream.engineer.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save raw dataframe in a csv\n",
    "bsi_stream.engineer.train_df.to_csv(\"./train_df_uvanew.csv\", index=False)\n",
    "bsi_stream.engineer.valid_df.to_csv(\"./valid_df_uvanew.csv\", index=False)\n",
    "bsi_stream.engineer.test_df.to_csv(\"./test_df_uvanew.csv\", index=False)\n",
    "bsi_stream.engineer.train_df_imputed.to_csv(\"./train_df_median_uvanew.csv\", index=False)\n",
    "bsi_stream.engineer.valid_df_imputed.to_csv(\"./valid_df_median_uvanew.csv\", index=False)\n",
    "bsi_stream.engineer.test_df_imputed.to_csv(\"./test_df_median_uvanew.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tfds in np.array objects\n",
    "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = bsi_stream.engineer.ExtractXY()\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "print(\"X_valid shape\", X_valid.shape)\n",
    "print(\"Y_valid shape\", Y_valid.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_test shape\", Y_test.shape)\n",
    "np.save(\"./X_train_uvanew.npy\", X_train)\n",
    "np.save(\"./Y_train_uvanew.npy\", Y_train)\n",
    "np.save(\"./X_valid_uvanew.npy\", X_valid)\n",
    "np.save(\"./Y_valid_uvanew.npy\", Y_valid)\n",
    "np.save(\"./X_test_uvanew.npy\", X_test)\n",
    "np.save(\"./Y_test_uvanew.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel classification -- by TFDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tfds = bsi_stream.engineer.train_tfds\n",
    "valid_tfds = bsi_stream.engineer.valid_tfds\n",
    "input_shape = list(train_tfds.element_spec[0].shape)[1:3]\n",
    "myMetrics = [\n",
    "    keras.metrics.AUC(name='AUROC', curve='ROC'),\n",
    "    keras.metrics.AUC(name='AUPRC', curve='PR')\n",
    "]\n",
    "\n",
    "keras.backend.clear_session()\n",
    "mdl = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=input_shape),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(3, activation='softmax'),\n",
    "    keras.layers.Reshape([1, -1])\n",
    "])\n",
    "mdl.summary()\n",
    "mdl.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=1e-3), metrics = myMetrics)\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "his = mdl.fit(train_tfds, \n",
    "              epochs=50, \n",
    "              validation_data=valid_tfds, callbacks=[callback])\n",
    "keras.backend.set_value(mdl.optimizer.learning_rate, 1e-4)\n",
    "his = mdl.fit(train_tfds, \n",
    "              epochs=50, \n",
    "              validation_data=valid_tfds, callbacks=[callback])\n",
    "keras.backend.set_value(mdl.optimizer.learning_rate, 1e-5)\n",
    "his = mdl.fit(train_tfds, \n",
    "              epochs=50, \n",
    "              validation_data=valid_tfds, callbacks=[callback])\n",
    "keras.backend.set_value(mdl.optimizer.learning_rate, 1e-6)\n",
    "his = mdl.fit(train_tfds, \n",
    "              epochs=50, \n",
    "              validation_data=valid_tfds, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification -- by DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = bsi_stream.engineer.ExtractXY()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], Y_train.shape[2])\n",
    "Y_valid = Y_valid.reshape(Y_valid.shape[0], Y_valid.shape[2])\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], Y_test.shape[2])\n",
    "Y_train = Y_train[:,1]\n",
    "Y_valid = Y_valid[:,1]\n",
    "Y_test = Y_test[:,1]\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "Y_valid = Y_valid.reshape(Y_valid.shape[0], 1)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
    "Y_all = np.concatenate((Y_train, Y_valid, Y_test), axis=0)\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "print(\"X_valid shape\", X_valid.shape)\n",
    "print(\"Y_valid shape\", Y_valid.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_test shape\", Y_test.shape)\n",
    "print(\"X_all shape\", X_all.shape)\n",
    "print(\"Y_all shape\", Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMetrics = [\n",
    "    keras.metrics.AUC(name='AUROC', curve='ROC'),\n",
    "    keras.metrics.AUC(name='AUPRC', curve='PR')\n",
    "]\n",
    "\n",
    "keras.backend.clear_session()\n",
    "mdl = keras.models.Sequential([\n",
    "    keras.layers.LSTM(16, return_sequences=True, input_shape=list(X_train.shape)[1:3]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(16, return_sequences=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "mdl.summary()\n",
    "mdl.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=1e-3), metrics = myMetrics)\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "his = mdl.fit(X_train, Y_train, \n",
    "              epochs=50, \n",
    "              validation_data=(X_valid,Y_valid), callbacks=[callback])\n",
    "keras.backend.set_value(mdl.optimizer.learning_rate, 1e-4)\n",
    "his = mdl.fit(X_train, Y_train, \n",
    "              epochs=50, \n",
    "              validation_data=(X_valid,Y_valid), callbacks=[callback])\n",
    "keras.backend.set_value(mdl.optimizer.learning_rate, 1e-5)\n",
    "his = mdl.fit(X_train, Y_train, \n",
    "              epochs=50, \n",
    "              validation_data=(X_valid,Y_valid), callbacks=[callback])\n",
    "keras.backend.set_value(mdl.optimizer.learning_rate, 1e-6)\n",
    "his = mdl.fit(X_train, Y_train, \n",
    "              epochs=50, \n",
    "              validation_data=(X_valid,Y_valid), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
